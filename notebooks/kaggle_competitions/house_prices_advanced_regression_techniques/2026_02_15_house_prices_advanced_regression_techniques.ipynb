{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# House Prices - Advanced Regression Techniques\n",
    "\n",
    "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "Kaggle competition for predicting house prices. What follows is my attempt a implementing a data science project from start to finish."
   ],
   "id": "815e752e5696a970"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:37.928844800Z",
     "start_time": "2026-02-19T14:30:37.571593200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_validate, learning_curve, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ],
   "id": "1528070e0b830ff4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Downloading the data\n",
    "\n",
    "Get the data from Kaggle. The section returns two dataframes:\n",
    "\n",
    "**df_train** and **df_test**"
   ],
   "id": "70c8d2ea3928994e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:38.166395300Z",
     "start_time": "2026-02-19T14:30:37.933843900Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zak\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:39.264644100Z",
     "start_time": "2026-02-19T14:30:38.177396900Z"
    }
   },
   "cell_type": "code",
   "source": "df_train = kagglehub.dataset_load(adapter=KaggleDatasetAdapter.PANDAS, handle='house-prices-advanced-regression-techniques', path='train.csv')",
   "id": "1de80f4fa569373",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:39.517398300Z",
     "start_time": "2026-02-19T14:30:39.370931800Z"
    }
   },
   "cell_type": "code",
   "source": "df_train",
   "id": "9f73daf9a570e96c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave   NaN      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave   NaN      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave   NaN      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave   NaN      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC  Fence MiscFeature MiscVal  \\\n",
       "0            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "2            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "3            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "4            Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "...          ...       ...  ...      ...    ...    ...         ...     ...   \n",
       "1455         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1456         Lvl    AllPub  ...        0    NaN  MnPrv         NaN       0   \n",
       "1457         Lvl    AllPub  ...        0    NaN  GdPrv        Shed    2500   \n",
       "1458         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "1459         Lvl    AllPub  ...        0    NaN    NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0         2   2008        WD         Normal     208500  \n",
       "1         5   2007        WD         Normal     181500  \n",
       "2         9   2008        WD         Normal     223500  \n",
       "3         2   2006        WD        Abnorml     140000  \n",
       "4        12   2008        WD         Normal     250000  \n",
       "...     ...    ...       ...            ...        ...  \n",
       "1455      8   2007        WD         Normal     175000  \n",
       "1456      2   2010        WD         Normal     210000  \n",
       "1457      5   2010        WD         Normal     266500  \n",
       "1458      4   2010        WD         Normal     142125  \n",
       "1459      6   2008        WD         Normal     147500  \n",
       "\n",
       "[1460 rows x 81 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 81 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:40.147651400Z",
     "start_time": "2026-02-19T14:30:39.564111900Z"
    }
   },
   "cell_type": "code",
   "source": "df_test = kagglehub.dataset_load(adapter=KaggleDatasetAdapter.PANDAS, handle='house-prices-advanced-regression-techniques', path='test.csv')",
   "id": "8eaf56db00309adf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:40.514048400Z",
     "start_time": "2026-02-19T14:30:40.379020300Z"
    }
   },
   "cell_type": "code",
   "source": "df_test",
   "id": "ce57f559bca2d86c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0     1461          20       RH         80.0    11622   Pave   NaN      Reg   \n",
       "1     1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
       "2     1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
       "3     1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
       "4     1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...   ...      ...   \n",
       "1454  2915         160       RM         21.0     1936   Pave   NaN      Reg   \n",
       "1455  2916         160       RM         21.0     1894   Pave   NaN      Reg   \n",
       "1456  2917          20       RL        160.0    20000   Pave   NaN      Reg   \n",
       "1457  2918          85       RL         62.0    10441   Pave   NaN      Reg   \n",
       "1458  2919          60       RL         74.0     9627   Pave   NaN      Reg   \n",
       "\n",
       "     LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence  \\\n",
       "0            Lvl    AllPub  ...         120        0    NaN  MnPrv   \n",
       "1            Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "2            Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
       "3            Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "4            HLS    AllPub  ...         144        0    NaN    NaN   \n",
       "...          ...       ...  ...         ...      ...    ...    ...   \n",
       "1454         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1455         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1456         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "1457         Lvl    AllPub  ...           0        0    NaN  MnPrv   \n",
       "1458         Lvl    AllPub  ...           0        0    NaN    NaN   \n",
       "\n",
       "     MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0            NaN       0      6    2010        WD         Normal  \n",
       "1           Gar2   12500      6    2010        WD         Normal  \n",
       "2            NaN       0      3    2010        WD         Normal  \n",
       "3            NaN       0      6    2010        WD         Normal  \n",
       "4            NaN       0      1    2010        WD         Normal  \n",
       "...          ...     ...    ...     ...       ...            ...  \n",
       "1454         NaN       0      6    2006        WD         Normal  \n",
       "1455         NaN       0      4    2006        WD        Abnorml  \n",
       "1456         NaN       0      9    2006        WD        Abnorml  \n",
       "1457        Shed     700      7    2006        WD         Normal  \n",
       "1458         NaN       0     11    2006        WD         Normal  \n",
       "\n",
       "[1459 rows x 80 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>RL</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>RM</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>700</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 80 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:40.853250Z",
     "start_time": "2026-02-19T14:30:40.812379Z"
    }
   },
   "cell_type": "code",
   "source": "len(df_train), len(df_test)",
   "id": "d18924efd3a38a88",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 1459)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection\n",
    "\n",
    "Using mutual_info_regression to decide which features to keep."
   ],
   "id": "bff19ca888f1d78c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:41.169608300Z",
     "start_time": "2026-02-19T14:30:41.157259300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores"
   ],
   "id": "336c2634f4f287db",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T14:30:41.644619200Z",
     "start_time": "2026-02-19T14:30:41.209511500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int\n",
    "\n",
    "\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores"
   ],
   "id": "4f1d499ee5e17fa4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zak\\AppData\\Local\\Temp\\ipykernel_3852\\2686959428.py:5: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  for colname in X.select_dtypes(\"object\"):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# All discrete features should now have integer dtypes (double-check this before using MI!)\u001B[39;00m\n\u001B[32m      9\u001B[39m discrete_features = X.dtypes == \u001B[38;5;28mint\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m mi_scores = \u001B[43mmake_mi_scores\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m mi_scores[::\u001B[32m3\u001B[39m]  \u001B[38;5;66;03m# show a few features with their MI scores\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mmake_mi_scores\u001B[39m\u001B[34m(X, y, discrete_features)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mmake_mi_scores\u001B[39m(X, y, discrete_features):\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     mi_scores = \u001B[43mmutual_info_regression\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m     mi_scores = pd.Series(mi_scores, name=\u001B[33m\"\u001B[39m\u001B[33mMI Scores\u001B[39m\u001B[33m\"\u001B[39m, index=X.columns)\n\u001B[32m      7\u001B[39m     mi_scores = mi_scores.sort_values(ascending=\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    213\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    214\u001B[39m         skip_parameter_validation=(\n\u001B[32m    215\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    216\u001B[39m         )\n\u001B[32m    217\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m218\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    224\u001B[39m     msg = re.sub(\n\u001B[32m    225\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    226\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    227\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    228\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:441\u001B[39m, in \u001B[36mmutual_info_regression\u001B[39m\u001B[34m(X, y, discrete_features, n_neighbors, copy, random_state, n_jobs)\u001B[39m\n\u001B[32m    325\u001B[39m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[32m    326\u001B[39m     {\n\u001B[32m    327\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m: [\u001B[33m\"\u001B[39m\u001B[33marray-like\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33msparse matrix\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    345\u001B[39m     n_jobs=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    346\u001B[39m ):\n\u001B[32m    347\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Estimate mutual information for a continuous target variable.\u001B[39;00m\n\u001B[32m    348\u001B[39m \n\u001B[32m    349\u001B[39m \u001B[33;03m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    439\u001B[39m \u001B[33;03m    array([0.117, 2.645, 0.0287])\u001B[39;00m\n\u001B[32m    440\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m441\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_estimate_mi\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    442\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    444\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdiscrete_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    445\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdiscrete_target\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    446\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_neighbors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    447\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    448\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    449\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    450\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:271\u001B[39m, in \u001B[36m_estimate_mi\u001B[39m\u001B[34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state, n_jobs)\u001B[39m\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_estimate_mi\u001B[39m(\n\u001B[32m    203\u001B[39m     X,\n\u001B[32m    204\u001B[39m     y,\n\u001B[32m   (...)\u001B[39m\u001B[32m    211\u001B[39m     n_jobs=\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    212\u001B[39m ):\n\u001B[32m    213\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Estimate mutual information between the features and the target.\u001B[39;00m\n\u001B[32m    214\u001B[39m \n\u001B[32m    215\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    269\u001B[39m \u001B[33;03m           Data Sets\". PLoS ONE 9(2), 2014.\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m271\u001B[39m     X, y = \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsc\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_numeric\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdiscrete_target\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    272\u001B[39m     n_samples, n_features = X.shape\n\u001B[32m    274\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(discrete_features, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbool\u001B[39m)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1314\u001B[39m, in \u001B[36mcheck_X_y\u001B[39m\u001B[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[39m\n\u001B[32m   1309\u001B[39m         estimator_name = _check_estimator_name(estimator)\n\u001B[32m   1310\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1311\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m requires y to be passed, but the target y is None\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1312\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1314\u001B[39m X = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1315\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1316\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1317\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1318\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1319\u001B[39m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1320\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1321\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1322\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1324\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1326\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1327\u001B[39m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1328\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1329\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1331\u001B[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001B[32m   1333\u001B[39m check_consistent_length(X, y)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1074\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1068\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1069\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFound array with dim \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marray.ndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1070\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m while dim <= 2 is required\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontext\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1071\u001B[39m     )\n\u001B[32m   1073\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ensure_all_finite:\n\u001B[32m-> \u001B[39m\u001B[32m1074\u001B[39m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1075\u001B[39m \u001B[43m        \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1077\u001B[39m \u001B[43m        \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1078\u001B[39m \u001B[43m        \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1079\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[32m   1082\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[32m   1083\u001B[39m         \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:133\u001B[39m, in \u001B[36m_assert_all_finite\u001B[39m\u001B[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[32m    131\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m133\u001B[39m \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\PyCharmProjects\\data-science\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:182\u001B[39m, in \u001B[36m_assert_all_finite_element_wise\u001B[39m\u001B[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[39m\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name == \u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[32m    167\u001B[39m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[32m    168\u001B[39m     msg_err += (\n\u001B[32m    169\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not accept missing values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    170\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    180\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m#estimators-that-handle-nan-values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    181\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m182\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[31mValueError\u001B[39m: Input X contains NaN."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## EDA\n",
    "\n",
    "What data do we have?\n",
    "Missing data?\n",
    "Text data?\n",
    "\n",
    "This section returns clean and ready to train data."
   ],
   "id": "494e185a3c69d599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EDA Step 1: The target\n",
    "\n",
    "Analyse the target variable."
   ],
   "id": "2b84fbb89d39b332"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train = df_train.select_dtypes('number')",
   "id": "18984d004f5626c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numerical_cols = df_train.select_dtypes('number')\n",
    "categorical_cols = df_train.select_dtypes('str')\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ],
   "id": "e8f2d7e9112cb7d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)"
   ],
   "id": "7d3bf68cec5c02d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.histplot(df_train.SalePrice)",
   "id": "675a45d057909b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution is not symmetrical\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "sns.histplot(np.log(df_train.SalePrice), ax=axes[0])\n",
    "\n",
    "# Useful when there are very small values for x. I don't think that applies in this case.\n",
    "sns.histplot(np.log1p(df_train.SalePrice), ax=axes[1])"
   ],
   "id": "462b49fe2771d4a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EDA Step 1: Missing data\n",
    "\n",
    "What data is missing? What are we going to do about it?\n",
    "\n",
    "Look at the distribution of the target. Is there a linear combination of the Does any transformation make the distribution symmetrical?\n",
    "\n"
   ],
   "id": "c4d11ddaf81a9874"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import r_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(working_memory=8192)"
   ],
   "id": "101d66e7e8788085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# handle the missing data\n",
    "# handle the categorical data\n",
    "# do this inside CV\n",
    "# pipeline\n",
    "# fit, transform"
   ],
   "id": "498e5af27dc56012",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train.isna().mean()\n",
    "\n",
    "# df_train.PoolQC\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "imputer.fit_transform(df_train)"
   ],
   "id": "ed9e714f353b4117",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "columns_with_missing_data = df_train.isna().mean()[df_train.isna().mean() > 0].reset_index()['index'].to_list()\n",
    "print(columns_with_missing_data)\n",
    "df_train_full_data = df_train.drop(columns_with_missing_data, axis=1)\n",
    "\n",
    "y_train = np.log(df_train_full_data.SalePrice) # Train the data on the log transformation of the data.\n",
    "X_train = df_train_full_data.select_dtypes('number').drop('SalePrice', axis=1)"
   ],
   "id": "eb68b3509319d171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_new = SelectKBest(r_regression, k=25).set_output(transform=\"pandas\").fit_transform(X_train, y_train)\n",
    "X_train_new"
   ],
   "id": "e8ea2f07017afb4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.scatter(y_train, y_train.reset_index().index)",
   "id": "b3ae51a48d7409f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.scatter(X_train_new.LotArea, np.log1p(y_train))",
   "id": "8e8ec1a07f2787fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.scatter(np.log1p(X_train_new.LotArea), np.log1p(y_train))",
   "id": "67c9d6c422b56c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Style configuration — used consistently throughout the notebook\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"muted\")\n",
    "PLOT_COLOR = \"#4878CF\"  # consistent blue for single-variable plots\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 5),\n",
    "    'axes.titlesize': 13,\n",
    "    'axes.labelsize': 11,\n",
    "    'font.size': 10\n",
    "})\n",
    "\n"
   ],
   "id": "d96daa211576b42e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Top row: Histograms with KDE\n",
    "sns.histplot(df_train['SalePrice'], kde=True, bins=50, color=PLOT_COLOR, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('SalePrice — Raw Distribution')\n",
    "axes[0, 0].set_xlabel('Sale Price ($)')\n",
    "\n",
    "sns.histplot(np.log1p(df_train['SalePrice']), kde=True, bins=50, color=PLOT_COLOR, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('SalePrice — Log-Transformed')\n",
    "axes[0, 1].set_xlabel('log(1 + Sale Price)')\n",
    "\n",
    "# Bottom row: Q-Q plots\n",
    "stats.probplot(df_train['SalePrice'], dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot — Raw')\n",
    "\n",
    "stats.probplot(np.log1p(df_train['SalePrice']), dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot — Log-Transformed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c7d179a92592a503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# sns.pairplot()",
   "id": "c62355c063481079",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training a model\n",
    "This section trains and refines the model."
   ],
   "id": "dde7225dda120ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Top 100 score: 0.11832\n",
    "# Top 1000 score: 0.12755\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "\n",
    "cv_results = cross_validate(LinearRegression(), X_train_new, y_train, scoring='neg_root_mean_squared_error', cv=5, return_train_score=True)\n",
    "\n",
    "print(cv_results)\n",
    "print(\"=\"*70)\n",
    "print(f\"train: {cv_results['train_score'].mean()} ± {cv_results['train_score'].std()}, test: {cv_results['test_score'].mean()} ± {cv_results['test_score'].std()}\")"
   ],
   "id": "70d9ed0f8f41faa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Run for a different number of features.",
   "id": "a42d4bd3dcbc7a38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.preprocessing import PolynomialFeatures",
   "id": "1e7a432f171d08b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Transform the data\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly_array = poly.fit_transform(X_train_new)\n",
    "\n",
    "# 2. Get the feature names\n",
    "# If X_train_new is a DataFrame, it pulls the actual names automatically\n",
    "feature_names = poly.get_feature_names_out(X_train_new.columns)\n",
    "\n",
    "# 3. Create the new DataFrame\n",
    "X_train_new_poly = pd.DataFrame(X_poly_array, columns=feature_names, index=X_train_new.index)\n",
    "\n",
    "X_train_new_poly"
   ],
   "id": "b1f8fdbf850a0c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This is too many features\n",
    "\n",
    "from statsmodels.regression.linear_model import OLS"
   ],
   "id": "fab58099465f8663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ols = OLS(y_train, X_train_new_poly).fit()\n",
    "ols.summary()\n",
    "\n",
    "feature_names = ols.params.index.tolist()\n",
    "pvalues = ols.pvalues\n",
    "\n",
    "\n",
    "\n",
    "top_features = pvalues[pvalues<0.05].index.tolist()\n",
    "\n",
    "X_train_new_poly[top_features]"
   ],
   "id": "84fd5b5440f113fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train.corr()",
   "id": "4deb4a7258068dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.heatmap(X_train.corr())",
   "id": "17595960099865b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_new_poly",
   "id": "eb65b8da5d6182c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_top_poly = SelectKBest(r_regression, k=25).set_output(transform=\"pandas\").fit_transform(X_train_new_poly, y_train)\n",
    "X_train_top_poly.corr()"
   ],
   "id": "d985759023cdbd57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_poly = cross_validate(LinearRegression(), X_train_new_poly, y_train, scoring='neg_root_mean_squared_error', cv=5, return_train_score=True)\n",
    "\n",
    "print(cv_results_poly)\n",
    "print(\"=\"*70)\n",
    "print(f\"train: {cv_results_poly['train_score'].mean()} ± {cv_results_poly['train_score'].std()}, test: {cv_results_poly['test_score'].mean()} ± {cv_results_poly['test_score'].std()}\")"
   ],
   "id": "80dfda7302f6d6f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_poly = cross_validate(LinearRegression(), X_train_new_poly[top_features], y_train, scoring='neg_root_mean_squared_error', cv=5, return_train_score=True)\n",
    "\n",
    "print(cv_results_poly)\n",
    "print(\"=\"*70)\n",
    "print(f\"train: {cv_results_poly['train_score'].mean()} ± {cv_results_poly['train_score'].std()}, test: {cv_results_poly['test_score'].mean()} ± {cv_results_poly['test_score'].std()}\")"
   ],
   "id": "bd46be5199cc02da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results_poly = cross_validate(LinearRegression(), X_train_top_poly, y_train, scoring='neg_root_mean_squared_error', cv=5, return_train_score=True)\n",
    "\n",
    "print(cv_results_poly)\n",
    "print(\"=\"*70)\n",
    "print(f\"train: {cv_results_poly['train_score'].mean()} ± {cv_results_poly['train_score'].std()}, test: {cv_results_poly['test_score'].mean()} ± {cv_results_poly['test_score'].std()}\")"
   ],
   "id": "42215add8b04e2db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from sklearn.linear_model import Ridge",
   "id": "db9b17066cbb7007",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$||y - Xw||^2_2 + alpha * ||w||^2_2$",
   "id": "e57d53e87fb7c9fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# alpha = 0 is the \"same\" as linear regression\n",
    "cv_results_poly = cross_validate(Ridge(alpha=1), X_train_new_poly, y_train, scoring='neg_root_mean_squared_error',\n",
    "                                 cv=5, return_train_score=True)\n",
    "\n",
    "print(cv_results_poly)\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    f\"train: {cv_results_poly['train_score'].mean()} ± {cv_results_poly['train_score'].std()}, test: {cv_results_poly['test_score'].mean()} ± {cv_results_poly['test_score'].std()}\")"
   ],
   "id": "8b891921fb56950a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train.min(), y_train.max()\n",
    "\n",
    "y_train"
   ],
   "id": "b22f124f9ee69b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prediction\n",
    "\n",
    "This section returns a csv file with a SalePrice column added to the test dataset."
   ],
   "id": "68ae2c1ad26d852e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "simple_lr = LinearRegression().fit(X_train_new, np.log1p(y_train))",
   "id": "563c08ef39938b96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_new.columns.to_list()",
   "id": "d2815f97bb190788",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test = df_test[X_train_new.columns.to_list()]\n",
    "len(X_test)"
   ],
   "id": "5a7f54998cc22854",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_test.fillna(0, inplace=True)",
   "id": "22bfa2115452014d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = simple_lr.predict(X_train_new)\n",
    "y_prices = np.exp(y_pred)-1\n",
    "plt.scatter(y_train, y_prices)\n",
    "plt.scatter(y_train, y_train)"
   ],
   "id": "17537ee10cbf018b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(y_prices), len(y_train)",
   "id": "9c7d3ee3040f66d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import plotly.express as px",
   "id": "ca9568a35e362939",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.scatter(x=y_train, y=y_prices)",
   "id": "31e5d9c028b55252",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.scatter(y_train - y_prices)",
   "id": "7795280ba4e76ce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "px.scatter((y_train - y_prices)/y_prices)",
   "id": "20c22d0176bb6ed1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sns.histplot((y_train - y_prices))",
   "id": "fe820c3c0f043ceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = simple_lr.predict(X_test)\n",
    "y_prices = np.exp(y_pred)-1\n",
    "y_prices"
   ],
   "id": "e9ce867e69ae1389",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Competition submission\n",
    "\n",
    "This section submits the data to Kaggle."
   ],
   "id": "f8ae60379d44601e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "submission_df = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': np.exp(y_prices)})\n",
    "submission_df"
   ],
   "id": "7f4c6c41b77528c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "submission_df.to_csv('submission.csv', index=False, header=True)",
   "id": "3f1558132fa41f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# 1. Authenticate\n",
    "KaggleApi().authenticate()"
   ],
   "id": "6173d1de3aed4f87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Define competition and submission details\n",
    "COMPETITION = 'house-prices-advanced-regression-techniques'\n",
    "\n",
    "def get_latest_score(competition):\n",
    "    # Fetch the list of all your submissions\n",
    "    submissions = KaggleApi().competition_submissions(competition)\n",
    "\n",
    "    if submissions:\n",
    "        latest = submissions[0]\n",
    "        # Status will be 'pending' while Kaggle is still calculating\n",
    "        return latest.public_score, latest.date, latest.status\n",
    "    return None"
   ],
   "id": "8970aa8e35433456",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_latest_score(COMPETITION)",
   "id": "5054a177823a2d77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"My first model submission\"",
   "id": "17889f1cb65e96b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!kaggle competitions submissions -c house-prices-advanced-regression-techniques",
   "id": "82d5b400a660b336",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!uv sync --upgrade",
   "id": "92a822476fc23c10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # 1. Create a Mixed-Type Dataset\n",
    "# # We create 1000 rows. 'cat_feature' is text, others are numbers.\n",
    "# data = pd.DataFrame({\n",
    "#     'num_1': np.random.normal(0, 1, 1000),\n",
    "#     'num_2': np.random.normal(5, 2, 1000), # Informative\n",
    "#     'num_3': np.random.normal(-5, 2, 1000),\n",
    "#     'cat_1': np.random.choice(['A', 'B', 'C'], 1000), # Categorical\n",
    "#     'cat_2': np.random.choice(['X', 'Y'], 1000)       # Categorical\n",
    "# })\n",
    "# # Target variable depends heavily on num_2 and cat_1\n",
    "# y = 3 * data['num_2'] + (data['cat_1'] == 'A') * 5 + np.random.normal(0, 1, 1000)\n",
    "#\n",
    "# # Introduce NaNs to test imputation\n",
    "# data.loc[::10, 'num_1'] = np.nan\n",
    "# data.loc[::10, 'cat_1'] = np.nan\n",
    "\n",
    "# 1. Define feature groups\n",
    "\n",
    "numeric_features = df_train.drop('SalePrice', axis=1).select_dtypes('number').columns.tolist()\n",
    "categorical_features = df_train.select_dtypes('str').columns.tolist()\n",
    "\n",
    "# 2. Build the Preprocessing Pipelines\n",
    "# Pipeline for Numbers: Impute Mean -> Scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for Categories: Impute 'missing' -> OneHotEncode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine them into a single Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False # Keeps names clean (e.g. 'cat_1_A' instead of 'cat__cat_1_A')\n",
    ")\n",
    "\n",
    "# 3. Create the Master Pipeline\n",
    "# Data Flow: Preprocessor (Clean/Encode) -> Selection -> Model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('selector', SelectKBest(mutual_info_regression, k=25)), # Select top 4 features\n",
    "    ('regressor', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# 4. Run Cross-Validation with Inspection\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = cross_validate(\n",
    "    pipeline,\n",
    "    df_train.drop('SalePrice', axis=1),\n",
    "    y=np.log(df_train['SalePrice']),\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,  # Give us training scores\n",
    "    return_estimator=True     # Give us the fitted pipeline for each fold\n",
    ")\n",
    "\n",
    "# 5. Extract and Display Results\n",
    "print(f\"{'Fold':<5} | {'Train RMSE':<10} | {'Test RMSE':<10} | {'Selected Features'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, estimator in enumerate(results['estimator']):\n",
    "    # Recover the feature names after OneHotEncoding\n",
    "    # The preprocessor is the first step in our pipeline\n",
    "    feature_names_out = estimator.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "    # Get the boolean mask of selected features from step 2 ('selector')\n",
    "    mask = estimator.named_steps['selector'].get_support()\n",
    "\n",
    "    # Filter the names\n",
    "    selected_feats = feature_names_out[mask]\n",
    "\n",
    "    # Calculate RMSEs\n",
    "    train_rmse = np.sqrt(-results['train_score'][i])\n",
    "    test_rmse = np.sqrt(-results['test_score'][i])\n",
    "\n",
    "    print(f\"{i+1:<5} | {train_rmse:.4f}     | {test_rmse:.4f}     | {list(selected_feats)}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Average Test RMSE: {np.sqrt(-results['test_score']).mean():.4f}\")"
   ],
   "id": "e5c51761652ed244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- CUSTOM TRANSFORMER: DROP CORRELATED FEATURES ---\n",
    "# This solves your concern about selected features being correlated.\n",
    "# We place this INSIDE the pipeline to prevent leakage.\n",
    "class DropCorrelatedFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.9):\n",
    "        self.threshold = threshold\n",
    "        self.to_drop = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Create correlation matrix\n",
    "        # Note: X might be a sparse matrix from OneHotEncoder, so we convert to dense\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            df = pd.DataFrame(X.toarray())\n",
    "        else:\n",
    "            df = pd.DataFrame(X)\n",
    "\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        self.to_drop = [column for column in upper.columns if any(upper[column] > self.threshold)]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            df = pd.DataFrame(X.toarray())\n",
    "        else:\n",
    "            df = pd.DataFrame(X)\n",
    "        return df.drop(df.columns[self.to_drop], axis=1).values\n",
    "\n",
    "# --- THE \"PACKAGE\": PIPELINE DOCTOR ---\n",
    "class PipelineDoctor:\n",
    "    def __init__(self, X, y, numerical_cols, categorical_cols):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_cols = numerical_cols\n",
    "        self.cat_cols = categorical_cols\n",
    "        self.pipeline = self._build_pipeline()\n",
    "\n",
    "    def _build_pipeline(self):\n",
    "        # 1. Preprocessing\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, self.num_cols),\n",
    "                ('cat', categorical_transformer, self.cat_cols)\n",
    "            ],\n",
    "            verbose_feature_names_out=False\n",
    "        )\n",
    "\n",
    "        # 2. Assembly\n",
    "        return Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('corr_filter', DropCorrelatedFeatures(threshold=0.85)), # NEW: Remove duplicates\n",
    "            ('selector', SelectKBest(mutual_info_regression, k=25)),\n",
    "            ('regressor', RandomForestRegressor(random_state=42, n_estimators=100))\n",
    "        ])\n",
    "\n",
    "    def diagnose_bias_variance(self):\n",
    "        \"\"\"Generates Learning Curves to detect Overfitting/Underfitting\"\"\"\n",
    "        print(\"\\n--- DIAGNOSIS 1: BIAS vs VARIANCE (Learning Curves) ---\")\n",
    "\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            self.pipeline, self.X, self.y, cv=5, n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 5), scoring='neg_mean_squared_error'\n",
    "        )\n",
    "\n",
    "        train_rmse = np.sqrt(-train_scores.mean(axis=1))\n",
    "        test_rmse = np.sqrt(-test_scores.mean(axis=1))\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_rmse, 'o-', color=\"r\", label=\"Training Score\")\n",
    "        plt.plot(train_sizes, test_rmse, 'o-', color=\"g\", label=\"Cross-Validation Score\")\n",
    "        plt.xlabel(\"Training Examples\")\n",
    "        plt.ylabel(\"RMSE (Lower is Better)\")\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid()\n",
    "        plt.show() #\n",
    "\n",
    "        # Automatic Advice\n",
    "        gap = test_rmse[-1] - train_rmse[-1]\n",
    "        print(f\"Final Train RMSE: {train_rmse[-1]:.4f}\")\n",
    "        print(f\"Final Test RMSE:  {test_rmse[-1]:.4f}\")\n",
    "        print(f\"Gap: {gap:.4f}\")\n",
    "\n",
    "        if train_rmse[-1] < 0.1 and gap > 0.2:\n",
    "            print(\">> DIAGNOSIS: HIGH VARIANCE (Overfitting).\")\n",
    "            print(\">> TIP: Reduce k in SelectKBest, increase regularization, or get more data.\")\n",
    "        elif train_rmse[-1] > 0.3:\n",
    "            print(\">> DIAGNOSIS: HIGH BIAS (Underfitting).\")\n",
    "            print(\">> TIP: Increase model complexity, add interaction terms, or increase k.\")\n",
    "        else:\n",
    "            print(\">> DIAGNOSIS: Model looks balanced.\")\n",
    "\n",
    "    def analyze_feature_stability(self):\n",
    "        \"\"\"Runs CV to see which features are consistently selected\"\"\"\n",
    "        print(\"\\n--- DIAGNOSIS 2: FEATURE STABILITY ---\")\n",
    "\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        results = cross_validate(\n",
    "            self.pipeline, self.X, self.y, cv=cv,\n",
    "            scoring='neg_mean_squared_error', return_estimator=True\n",
    "        )\n",
    "\n",
    "        # Aggregate selected features\n",
    "        feature_counts = {}\n",
    "\n",
    "        for estimator in results['estimator']:\n",
    "            # Get feature names from preprocessor\n",
    "            all_names = estimator.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "            # The correlation filter drops columns, so we need to track what remains\n",
    "            # This is tricky because custom transformers don't always expose feature names easily\n",
    "            # For this demo, we assume the correlation filter keeps names aligned or we rely on the selector mask\n",
    "            # Note: In production, you'd implement get_feature_names_out in DropCorrelatedFeatures\n",
    "\n",
    "            # Simple approach: Mask from Selector\n",
    "            mask = estimator.named_steps['selector'].get_support()\n",
    "\n",
    "            # Note: This logic assumes 1-to-1 mapping.\n",
    "            # If DropCorrelatedFeatures drops columns, the mask length will differ from all_names length.\n",
    "            # To keep this robust for the demo, we will check indices.\n",
    "\n",
    "            # (Simplified for display purposes - capturing the raw indices selected)\n",
    "            # In a real package, you would rigorously track column names through steps.\n",
    "            pass\n",
    "\n",
    "        print(f\"Average Test RMSE across folds: {np.sqrt(-results['test_score']).mean():.4f}\")\n",
    "        print(\">> TIP: If your CV score standard deviation is high, your feature selection is unstable.\")\n",
    "\n",
    "# --- USAGE EXAMPLE ---\n",
    "# Assuming you have df_train loaded from Kaggle\n",
    "# df_train = kagglehub.dataset_load(...) or pd.read_csv('train.csv')\n",
    "\n",
    "# Dummy data for demonstration\n",
    "\n",
    "\n",
    "# 1. Setup Data\n",
    "X = df_train.drop('SalePrice', axis=1)\n",
    "y = np.log(df_train['SalePrice'])\n",
    "\n",
    "# Fix the dtypes selector (Pandas often loads strings as 'object')\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2. Run the Doctor\n",
    "doc = PipelineDoctor(X, y, num_cols, cat_cols)\n",
    "doc.diagnose_bias_variance()\n",
    "doc.analyze_feature_stability()"
   ],
   "id": "301bd21720893fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "766c9be3bac306c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
