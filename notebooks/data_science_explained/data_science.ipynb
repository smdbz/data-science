{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Data Science\n",
    "\n",
    "This notebook is a place to consolidate my learning of data science techniques and a reference for when I forget things. I've tried to explain everything in my own words so when I review it, it'll make sense to me."
   ],
   "id": "99da2584dff00850"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Laying the foundations\n",
    "\n",
    "For me at least, the maths is not optional.\n",
    "\n",
    "At each stage it's important to define the specific problem being solved. Whilst each step builds on the next, it's important to remember the context for the solution as what applies at the start doesn't necessarily apply at the end."
   ],
   "id": "39b85fbf529886fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Univariate Linear Regression\n",
    "\n",
    "AKA linear regression with one variable.\n",
    "\n",
    "This is a good place to start.\n",
    "\n",
    "A simple 2D plot with a input on the x axis and a prediction on the y axis.\n",
    "\n"
   ],
   "id": "be8e1687d0037c0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b9ee3f5f4f583b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T20:47:18.369844100Z",
     "start_time": "2026-01-15T20:47:18.355497100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# TODO: What is numpy anyway?\n",
    "# Create a section on linear algebra and how it is used in numpy to calculate \"things\" efficiently.\n",
    "\n",
    "# Create a section on calculus to better describe the cost function and gradient descent.\n",
    "# Start by solving things algebraically, then show how this quickly breaks down and gradient descent only requires the partial derivative.\n",
    "\n",
    "# When are things for certain a convex function and when can you not be sure?\n",
    "# When is it possible to know if you are in a global minimum vs a local minimum?"
   ],
   "id": "1d57134ecdc9d03e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T22:40:56.530075200Z",
     "start_time": "2026-01-15T22:40:56.518826400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = np.array([1.0, 2.0])   #(size in 1000 square feet)\n",
    "y_train = np.array([300.0, 500.0])"
   ],
   "id": "35113c15c9a61bac",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T22:40:57.327127300Z",
     "start_time": "2026-01-15T22:40:57.319608100Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3859c5d40b2aa7c5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Regression with multiple input variables\n",
    "\n",
    "Stop thinking about data as a table. Instead, think of a row as a vector in a matrix.\n",
    "\n",
    "We are basically saying that any given target variable is the dot product of the input variables and some, to be calculated weights.\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b $$\n",
    "\n",
    "Or in the expanded form:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b $$\n",
    "\n",
    "$\\mathbf{x}$ is a vector or row of values from a table.\n"
   ],
   "id": "8a410692348fc112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "w = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "x = np.array([10, 20, 30])"
   ],
   "id": "46d54c5bfb555852"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "f = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + b\n",
    "f"
   ],
   "id": "76e6a56cc9636790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total = 0\n",
    "for i in range(len(w)):\n",
    "    total += w[i]*x[i]\n",
    "\n",
    "total + b"
   ],
   "id": "c508d8dd8e03eb33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vectorization",
   "id": "a4f4de5469cfa43b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T02:38:20.534272100Z",
     "start_time": "2026-01-16T02:38:20.516889500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f = np.dot(w, x) + b\n",
    "f"
   ],
   "id": "51b9a7621ec393f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-35.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What if I think my target variable scales with an x^2 term?\n",
    "\n",
    "Polynomials are \"Linear in Parameters\"\n",
    "Even though a polynomial like $y = w_2x^2 + w_1x + b$ in non-linear with respect to x, it is linear with respect to the coefficients $(w_2, w_1, b)$.\n",
    "\n",
    "If each power of x is treated as a separate feature e.g. $z_1 = x, z_2 = x^2$, the equation becomes $y = w_2z + w_1x + b$.\n",
    "\n",
    "Using ordinary least squares guarantees that you'll find the minimum value, the"
   ],
   "id": "cfc1d3b5144a3fa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b05302649b75bd1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
